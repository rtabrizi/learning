{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = 1 + counts.get(label, 0)\n",
    "    probs = [count / len(y) * np.log2(count / len(y)) for count in counts.values() if len(y) > 0]\n",
    "    return - np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./dataset/titanic/titanic_training.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 230\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Load titanic data\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     path_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/titanic/titanic_training.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 230\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     path_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/titanic/titanic_test_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    232\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m genfromtxt(path_test, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py:1969\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py?line=1966'>1967</a>\u001b[0m     fname \u001b[39m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py?line=1967'>1968</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py?line=1968'>1969</a>\u001b[0m     fid \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py?line=1969'>1970</a>\u001b[0m     fid_ctx \u001b[39m=\u001b[39m contextlib\u001b[39m.\u001b[39mclosing(fid)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/npyio.py?line=1970'>1971</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=155'>156</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=156'>157</a>\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=157'>158</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=188'>189</a>\u001b[0m \n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=189'>190</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=191'>192</a>\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=192'>193</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=529'>530</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=530'>531</a>\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=531'>532</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/189/lib/python3.9/site-packages/numpy/lib/_datasource.py?line=532'>533</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./dataset/titanic/titanic_training.csv not found."
     ]
    }
   ],
   "source": [
    "# You may want to install \"gprof2dot\"\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import math\n",
    "import pydot\n",
    "\n",
    "eps = 1e-5  # a small number\n",
    "np.random.seed(2147483647)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    # @staticmethod\n",
    "    def information_gain(self, X, y, thresh):\n",
    "        # TODO: implement information gain function\n",
    "        left_idxs = np.argwhere(X <= X).flatten()\n",
    "        right_idxs = np.argwhere(X > X).flatten()\n",
    "\n",
    "        y0, y1 = y[left_idxs], y[right_idxs]\n",
    "\n",
    "        if len(y0) == 0 or len(y1) == 0:\n",
    "            return 0\n",
    "\n",
    "        parent_entropy = self.entropy(y)\n",
    "        left_entropy, right_entropy = self.gini_impurity(X, y, thresh)\n",
    "        \n",
    "        left_w = len(y0) / len(y)\n",
    "        right_w = len(y1) / len(y)\n",
    "        assert(int(left_w + right_w) == 1)\n",
    "        print(\"ENTROPY\")\n",
    "        print(parent_entropy, left_entropy, right_entropy)\n",
    "        gain = parent_entropy - (left_w + left_entropy + right_w + right_entropy)\n",
    "        return gain\n",
    "        \n",
    "    \n",
    "    def entropy(self, y):\n",
    "        counts = {}\n",
    "        for label in y:\n",
    "            counts[label] = 1 + counts.get(label, 0)\n",
    "        card = len(y)\n",
    "        entropy = 0\n",
    "        for label, count in counts.items():\n",
    "            p = count / card\n",
    "            res = p * np.log2(p)\n",
    "            entropy -= res\n",
    "        return entropy\n",
    "\n",
    "    # @staticmethod\n",
    "    def gini_impurity(self,X, y, thresh):\n",
    "        # TODO: implement gini impurity function\n",
    "        _, y0, _, y1 = self.split(X, y, idx=0, thresh=thresh)\n",
    "        left_entropy = self.entropy(y0)\n",
    "        right_entropy = self.entropy(y1)\n",
    "        return left_entropy, right_entropy\n",
    "        \n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            thresh = np.array([\n",
    "                np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)\n",
    "                for i in range(X.shape[1])\n",
    "            ])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "            # print(gains)\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y, keepdims=True).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y, keepdims=True).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "\n",
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            sklearn.tree.DecisionTreeClassifier(random_state=i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: implement function\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        # TODO: implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "class BoostedRandomForest(RandomForest):\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        # TODO: implement function\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == ''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == '-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i], keepdims=True).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
    "        print(\"First splits\", first_splits)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = './dataset/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "        path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "        y = data[1:, -1]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "        labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "        y = np.array(y[labeled_idx])\n",
    "        y = y.astype(float).astype(int)\n",
    "\n",
    "\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, :-1]) + onehot_features\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = './dataset/spam/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "    print(\"\\n\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "    # Basic decision tree\n",
    "    print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "    dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "    print(\"Predictions\", dt.predict(Z)[:100])\n",
    "\n",
    "    print(\"\\n\\nPart (c): sklearn's decision tree\")\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)\n",
    "    clf.fit(X, y)\n",
    "    evaluate(clf)\n",
    "    out = io.StringIO()\n",
    "\n",
    "    # You may want to install \"gprof2dot\"\n",
    "    sklearn.tree.export_graphviz(\n",
    "        clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "    graph = pydot.graph_from_dot_data(out.getvalue())\n",
    "    pydot.graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: implement and evaluate!\n",
    "    print(\"\\n\\nPart (d): Vanilla Decision Tree\")\n",
    "    tree = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    evaluate(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
