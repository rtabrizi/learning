{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# class for tokenizer and sequence classification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9429818391799927}]\n"
     ]
    }
   ],
   "source": [
    "# simple pretrained classifier\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "res = classifier(\"How are you doing today?\")\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find different models [here](\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_pretrained(model_name): load pretrained model for any architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "res = classifier(\"How are you doing today?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe that input_ids includes 101 and 102, which indicate the beginning and end respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', 'hugging', '##face', 'transformers', 'library']\n",
      "Token IDs: [2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 17662, 12172, 19081, 3075]\n",
      "Input IDs: {'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 17662, 12172, 19081, 3075, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"We are very happy to show you the huggingface transformers library\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"We are very happy to show you the huggingface transformers library\")\n",
    "\n",
    "print(\"Tokens: %s\" % tokens)\n",
    "print(\"Token IDs: %s\" % token_ids)\n",
    "print(\"Input IDs: %s\" % input_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad or truncate all sample points for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"We are very happy to show you the HuggingFace Transformers library\",\n",
    "            \"We hope you don't hate it.\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automodel lets you add labels=torch.tensor([1,0])\n",
    "which outputs loss for SequenceClassifierOutput\n",
    "\n",
    "Below is the manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.3168), logits=tensor([[-4.0383,  4.2478],\n",
      "        [ 0.0818, -0.0418]]), hidden_states=None, attentions=None)\n",
      "tensor([[2.5194e-04, 9.9975e-01],\n",
      "        [5.3086e-01, 4.6914e-01]])\n",
      "tensor([1, 0])\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # batch is a dictionary, so we unpack them\n",
    "    outputs = model(**batch, labels=torch.tensor([1, 0]))\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the Model\n",
    "save your finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"saved\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading in a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppose we want to classify texts in German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 161/161 [00:00<00:00, 26.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 149kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 255k/255k [00:00<00:00, 5.09MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 59.6kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 436M/436M [01:28<00:00, 4.95MB/s] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'returns_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m X_train \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2527'>2528</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2528'>2529</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2529'>2530</a>\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2530'>2531</a>\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2531'>2532</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2616\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2610'>2611</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2611'>2612</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2612'>2613</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2613'>2614</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2614'>2615</a>\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2615'>2616</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2616'>2617</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2617'>2618</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2618'>2619</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2619'>2620</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2620'>2621</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2621'>2622</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2622'>2623</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2623'>2624</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2624'>2625</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2625'>2626</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2626'>2627</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2627'>2628</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2628'>2629</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2629'>2630</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2630'>2631</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2631'>2632</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2632'>2633</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2633'>2634</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2634'>2635</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2635'>2636</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2636'>2637</a>\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2637'>2638</a>\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2653'>2654</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2654'>2655</a>\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2807\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2796'>2797</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2797'>2798</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2798'>2799</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2799'>2800</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2803'>2804</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2804'>2805</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2806'>2807</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2807'>2808</a>\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2808'>2809</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2809'>2810</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2810'>2811</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2811'>2812</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2812'>2813</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2813'>2814</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2814'>2815</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2815'>2816</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2816'>2817</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2817'>2818</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2818'>2819</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2819'>2820</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2820'>2821</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2821'>2822</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2822'>2823</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2823'>2824</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2824'>2825</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'returns_tensors'"
     ]
    }
   ],
   "source": [
    "model_name = \"oliverguhr/german-sentiment-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train = [\"\", \"\", \"\"]\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, returns_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    label_ids = torch.argmax(outputs.logits, dim=1)\n",
    "    print(label_ids)\n",
    "    labels = [model.config.id2label[label_id] for label_id in label_ids.tolist()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning our own models\n",
    "[helpful link!](https://huggingface.co/transformers/v3.2.0/custom_datasets.html)\n",
    "\n",
    "### general approach: \n",
    "1. prepare dataset\n",
    "2. load pretrained tokenizer, call it with dataset -> encoding\n",
    "3. build pytorch dataset with encodings\n",
    "4. load pretrained model\n",
    "5. \n",
    "    a) load trainer and train it\n",
    "    b)  or use native pytorch training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
